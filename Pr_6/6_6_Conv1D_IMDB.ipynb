{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPex7r4hB+WV5LV/EkbMTmX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/natsakh/IAD/blob/main/Pr_6/6_6_Conv1D_IMDB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import re, string\n",
        "from collections import Counter\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
        "import random\n",
        "import torch.nn.functional as F\n",
        "\n",
        "torch.manual_seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5c3tJeq44UtG",
        "outputId": "1ffbcbd2-b1d2-4d1d-be83-e11fe94b1652"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hugging Face Datasets — бібліотека для завантаження датасетів.\n",
        "#У Google Colab вона вже попередньо встановлена\n",
        "# Якщо ви працюєте локально — попередньо виконайте:\n",
        "#     pip install datasets\n",
        "from datasets import load_dataset\n",
        "#https://huggingface.co/datasets"
      ],
      "metadata": {
        "id": "LuCCdUwkATes"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = load_dataset(\"imdb\")\n",
        "#ds[\"train\"], ds[\"test\"]\n",
        "#список словників — кожен елемент має ключі 'text' і 'label'"
      ],
      "metadata": {
        "id": "dxkvdrcw_vWb"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    text = text.lower()                                       # до нижнього регістру\n",
        "    text = re.sub(r\"@\\S+\", \" \", text)                         # прибрати згадки @user\n",
        "    text = re.sub(r\"http\\S+\", \" \", text)                      # прибрати посилання\n",
        "    text = re.sub(r\"[^a-z\\s]\", \" \", text)                     # залишити лише букви (прибрати цифри, спецсимволи)\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)  # прибрати пунктуацію\n",
        "    text = re.sub(r\"\\s+\", \" \", text)                          # замінити багато пробілів на один\n",
        "    return text"
      ],
      "metadata": {
        "id": "KI9DmAi0JvDt"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_texts = [clean_text(ex[\"text\"]) for ex in ds[\"train\"]]\n",
        "tokens = [word_tokenize(t) for t in cleaned_texts]"
      ],
      "metadata": {
        "id": "tJwNUD7WJ8p1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "counter = Counter()\n",
        "for tok_list in tokens:\n",
        "    counter.update(tok_list)"
      ],
      "metadata": {
        "id": "OTnFpSWP5O_D"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# кількість унікальних слів\n",
        "vocab_size = len(counter)\n",
        "\n",
        "print(f\"Кількість унікальних слів у train-наборі: {vocab_size:,}\")\n",
        "\n",
        "MAX_VOCAB = 20_000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "301g3yaR5pgK",
        "outputId": "5ec758d9-b8a9-47d8-98fa-aea808996f65"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Кількість унікальних слів у train-наборі: 73,206\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# створюємо списки перетворень, створюємо словник\n",
        "specials = [\"[PAD]\", \"[UNK]\"]\n",
        "most_common = counter.most_common(MAX_VOCAB - len(specials))\n",
        "itos = specials + [w for w, _ in most_common]       # index → string\n",
        "stoi = {w: i for i, w in enumerate(itos)}           # string → index\n",
        "\n",
        "PAD_IDX = stoi[\"[PAD]\"]\n",
        "UNK_IDX = stoi[\"[UNK]\"]\n",
        "\n",
        "print(\"Розмір словника:\", len(stoi))\n",
        "print(\"Приклад індексу:\", stoi.get(\"movie\", UNK_IDX))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hhmoio6xM03x",
        "outputId": "b6a2f99f-337e-4367-d020-04c9953ca2cc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Розмір словника: 20000\n",
            "Приклад індексу: 19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#кодуємо тексти у послідовності індексів\n",
        "#для кожного токена беремо його індекс зі словника stoi,\n",
        "#якщо слова немає — повертаємо індекс UNK_IDX.\n",
        "def encode(tokens):\n",
        "    return [stoi.get(t, UNK_IDX) for t in tokens]\n",
        "\n",
        "encoded_texts = [encode(tok_list) for tok_list in tokens]\n",
        "print(encoded_texts[0][:20])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBtz4NNAYUJ_",
        "outputId": "a835a95c-95fe-4d58-d89f-8d029d847b6f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[11, 1595, 11, 240, 1973, 4046, 39, 61, 367, 1098, 87, 5, 32, 2, 6975, 13, 3357, 9, 55, 9]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Padding (вирівнюємо довжину)\n",
        "#Нейронна мережа очікує тензори однакової довжини,\n",
        "#тому короткі тексти “доповнюємо” <pad>, а надто довгі — обрізаємо\n",
        "\n",
        "MAX_LEN = 100   # довжина послідовності\n",
        "def pad_sequence(seq):\n",
        "    seq = seq[:MAX_LEN] + [PAD_IDX] * max(0, MAX_LEN - len(seq))\n",
        "    return torch.tensor(seq, dtype=torch.long)\n",
        "\n",
        "X = torch.stack([pad_sequence(seq) for seq in encoded_texts])\n",
        "print(X.shape)   # [N, MAX_LEN]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjWh1MuMY5Xl",
        "outputId": "5c4b9a44-c5e8-49ac-d820-c130d8789823"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([25000, 100])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# мітки (labels)\n",
        "y = torch.tensor([ex[\"label\"] for ex in ds[\"train\"]], dtype=torch.float32)\n",
        "print(y.shape)   # [N]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JdVlkiH-ZSho",
        "outputId": "0cec9088-ae23-4fdc-d881-8e01ecef2f0d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([25000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TensorDataset і DataLoader\n",
        "dataset = TensorDataset(X, y)\n",
        "\n",
        "# розділяємо на train/val\n",
        "VAL_FRAC = 0.2\n",
        "val_sz = int(len(dataset) * VAL_FRAC)\n",
        "train_sz = len(dataset) - val_sz\n",
        "train_ds, val_ds = random_split(dataset, [train_sz, val_sz],\n",
        "                                generator=torch.Generator().manual_seed(42))\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True)\n",
        "val_loader   = DataLoader(val_ds, batch_size=128)\n"
      ],
      "metadata": {
        "id": "RfTqUNasZcy0"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# використовуємо ті самі clean_text, токенайзер/word_tokenize, stoi, PAD_IDX, MAX_LEN\n",
        "\n",
        "# Підготовка X_test, y_test\n",
        "cleaned_test = [clean_text(ex[\"text\"]) for ex in ds[\"test\"]]\n",
        "tokens_test  = [word_tokenize(t) for t in cleaned_test]\n",
        "encoded_test = [[stoi.get(tok, UNK_IDX) for tok in toks] for toks in tokens_test]\n",
        "\n",
        "def pad_sequence(seq):\n",
        "    seq = seq[:MAX_LEN] + [PAD_IDX] * max(0, MAX_LEN - len(seq))\n",
        "    return torch.tensor(seq, dtype=torch.long)\n",
        "\n",
        "X_test = torch.stack([pad_sequence(seq) for seq in encoded_test])\n",
        "y_test = torch.tensor([ex[\"label\"] for ex in ds[\"test\"]], dtype=torch.float32)\n",
        "\n",
        "# 2) TensorDataset + DataLoader\n",
        "test_ds = TensorDataset(X_test, y_test)\n",
        "test_loader = DataLoader(test_ds, batch_size=128, shuffle=False)  # на тесті shuffle=False\n"
      ],
      "metadata": {
        "id": "FYz9uqj0hgLC"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(x)\n",
        "        loss = criterion(out, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "    train_loss /= len(loader)\n",
        "\n",
        "    return train_loss\n"
      ],
      "metadata": {
        "id": "m5crVj8tAi59"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_one_epoch(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    valid_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for x, y in loader:\n",
        "          x, y = x.to(device), y.to(device)\n",
        "          out = model(x)\n",
        "          loss = criterion(out, y)\n",
        "          valid_loss += loss.item()\n",
        "    valid_loss /= len(loader)\n",
        "\n",
        "    return valid_loss"
      ],
      "metadata": {
        "id": "szuIUjuXAonL"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Convolutional Neural Networks for Sentence Classification - https://arxiv.org/abs/1408.5882\n",
        "class TextCNN(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        emb_dim: int = 128,\n",
        "        num_filters: int = 128,\n",
        "        kernel_sizes=(3, 4, 5),\n",
        "        num_classes: int = 1,        # 1 для бінарної класифікації (BCEWithLogitsLoss)\n",
        "        pad_idx: int = 0,\n",
        "        dropout: float = 0.5,\n",
        "        use_pretrained_weight: torch.Tensor | None = None,\n",
        "        freeze_emb: bool = False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        # Embedding\n",
        "        if use_pretrained_weight is None:\n",
        "            self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n",
        "        else:\n",
        "            self.embedding = nn.Embedding.from_pretrained(\n",
        "                use_pretrained_weight, freeze=freeze_emb, padding_idx=pad_idx\n",
        "            )\n",
        "\n",
        "        # Набір 1D-конволюцій з різними kernel sizes (in_channels=emb_dim, out_channels=num_filters)\n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Conv1d(in_channels=emb_dim, out_channels=num_filters, kernel_size=k)\n",
        "            for k in kernel_sizes\n",
        "        ])\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(num_filters * len(kernel_sizes), num_classes)\n",
        "\n",
        "    def forward(self, x):                 # x: [B, T] (індекси слів)\n",
        "        emb = self.embedding(x)           # [B, T, E]\n",
        "        emb = emb.transpose(1, 2)         # → [B, E, T] для Conv1d\n",
        "\n",
        "        # Conv1d → ReLU → глобальний MaxPool по часовій осі\n",
        "        conv_feats = []\n",
        "        for conv in self.convs:\n",
        "            # вихід: [B, C, T'] де T' = T - k + 1\n",
        "            h = F.relu(conv(emb))\n",
        "            # global max-over-time: [B, C]\n",
        "            h = F.max_pool1d(h, kernel_size=h.size(2)).squeeze(2)\n",
        "            conv_feats.append(h)\n",
        "\n",
        "        z = torch.cat(conv_feats, dim=1)  # [B, C * len(K)]\n",
        "        z = self.dropout(z)\n",
        "        logits = self.fc(z)               # [B, num_classes]\n",
        "        # Для BCEWithLogitsLoss повертаємо [B]; для CE — залишаємо [B, num_classes]\n",
        "        return logits.squeeze(1) if logits.size(1) == 1 else logits\n"
      ],
      "metadata": {
        "id": "u8vCiwqrBjd2"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(stoi)\n",
        "\n",
        "model = TextCNN(\n",
        "    vocab_size=len(stoi),\n",
        "    emb_dim=128,\n",
        "    num_filters=128,\n",
        "    kernel_sizes=(3,4,5),\n",
        "    num_classes=1,\n",
        "    pad_idx=PAD_IDX,\n",
        "    dropout=0.5,\n",
        ").to(device)\n",
        "print(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2Tr1vNVdAi2",
        "outputId": "4bf2d7b3-9d57-4be7-f29a-53f8cd886353"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TextCNN(\n",
            "  (embedding): Embedding(20000, 128, padding_idx=0)\n",
            "  (convs): ModuleList(\n",
            "    (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,))\n",
            "    (1): Conv1d(128, 128, kernel_size=(4,), stride=(1,))\n",
            "    (2): Conv1d(128, 128, kernel_size=(5,), stride=(1,))\n",
            "  )\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Linear(in_features=384, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "jhWlOMnLc0Kk"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# навчання\n",
        "n_epochs = 7\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "   train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
        "   val_loss = evaluate_one_epoch(model, val_loader, criterion, device)\n",
        "\n",
        "   print(f\"[{epoch+1:02d}] train_loss={train_loss:.4f} | val_loss={val_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVvbS27H0fq8",
        "outputId": "9fb01eb2-a5bb-4c21-f08a-3ea52c972bc0"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[01] train_loss=0.6730 | val_loss=0.5751\n",
            "[02] train_loss=0.5551 | val_loss=0.4971\n",
            "[03] train_loss=0.4891 | val_loss=0.4607\n",
            "[04] train_loss=0.4335 | val_loss=0.4347\n",
            "[05] train_loss=0.3773 | val_loss=0.4180\n",
            "[06] train_loss=0.3185 | val_loss=0.4037\n",
            "[07] train_loss=0.2744 | val_loss=0.4107\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Для фінальної перевірки якості\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss, total_acc, n = 0.0, 0, 0\n",
        "    for xb, yb in loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        logits = model(xb)\n",
        "        loss = criterion(logits, yb)\n",
        "        preds = (torch.sigmoid(logits) >= 0.5).long()\n",
        "        total_loss += loss.item() * xb.size(0)\n",
        "        total_acc  += (preds == yb.long()).sum().item()\n",
        "        n += xb.size(0)\n",
        "    return total_loss/n, total_acc/n"
      ],
      "metadata": {
        "id": "dusn5wVifbp3"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
        "print(f\"TEST  loss={test_loss:.4f}  acc={test_acc*100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NRb7i14_hvo-",
        "outputId": "cf935dfd-604e-499c-c294-3cdf12a36d86"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TEST  loss=0.4388  acc=79.85%\n"
          ]
        }
      ]
    }
  ]
}