{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/natsakh/IAD/blob/main/Pr_7/7_3_Transformer_Decoder_IMDB_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5c3tJeq44UtG",
        "outputId": "fe9a8a8c-8291-47f8-db50-5508f8721d04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import re, string\n",
        "from collections import Counter\n",
        "\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
        "import random\n",
        "\n",
        "torch.manual_seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "LuCCdUwkATes"
      },
      "outputs": [],
      "source": [
        "# Hugging Face Datasets — бібліотека для завантаження датасетів.\n",
        "#У Google Colab вона вже попередньо встановлена\n",
        "# Якщо ви працюєте локально — попередньо виконайте:\n",
        "#     pip install datasets\n",
        "from datasets import load_dataset\n",
        "#https://huggingface.co/datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "dxkvdrcw_vWb"
      },
      "outputs": [],
      "source": [
        "ds = load_dataset(\"imdb\")\n",
        "#ds[\"train\"], ds[\"test\"]\n",
        "#список словників — кожен елемент має ключі 'text' і 'label'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "KI9DmAi0JvDt"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    text = text.lower()                                       # до нижнього регістру\n",
        "    text = re.sub(r\"@\\S+\", \" \", text)                         # прибрати згадки @user\n",
        "    text = re.sub(r\"http\\S+\", \" \", text)                      # прибрати посилання\n",
        "    text = re.sub(r\"<.*?>\", \" \", text)                        # прибрати HTML-теги\n",
        "    text = re.sub(r\"[^a-z\\s]\", \" \", text)                     # залишити лише букви (прибрати цифри, спецсимволи)\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)  # прибрати пунктуацію\n",
        "    text = re.sub(r\"\\s+\", \" \", text)                          # замінити багато пробілів на один\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "tJwNUD7WJ8p1"
      },
      "outputs": [],
      "source": [
        "cleaned_texts = [clean_text(ex[\"text\"]) for ex in ds[\"train\"]]\n",
        "tokens = [word_tokenize(t) for t in cleaned_texts]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "OTnFpSWP5O_D"
      },
      "outputs": [],
      "source": [
        "counter = Counter()\n",
        "for tok_list in tokens:\n",
        "    counter.update(tok_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "301g3yaR5pgK",
        "outputId": "3ae0ed3d-dcfd-4ec3-8dc6-9c78ea7578d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Кількість унікальних слів у train-наборі: 73,206\n"
          ]
        }
      ],
      "source": [
        "# кількість унікальних слів\n",
        "vocab_size = len(counter)\n",
        "\n",
        "print(f\"Кількість унікальних слів у train-наборі: {vocab_size:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dpc4NmcD7GeM",
        "outputId": "a2f1b238-62b6-4887-c4f2-524448e4de02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Розмір словника: 20000\n",
            "PAD: 0 UNK: 1 BOS: 2 EOS: 3\n"
          ]
        }
      ],
      "source": [
        " # обмежимо словник\n",
        "MAX_VOCAB = 20_000\n",
        "\n",
        "specials = [\"[PAD]\", \"[UNK]\", \"[BOS]\", \"[EOS]\"]\n",
        "most_common = counter.most_common(MAX_VOCAB - len(specials))\n",
        "itos = specials + [w for w, _ in most_common]\n",
        "stoi = {w: i for i, w in enumerate(itos)}\n",
        "\n",
        "PAD_IDX = stoi[\"[PAD]\"]\n",
        "UNK_IDX = stoi[\"[UNK]\"]\n",
        "BOS_IDX = stoi[\"[BOS]\"]\n",
        "EOS_IDX = stoi[\"[EOS]\"]\n",
        "\n",
        "print(\"Розмір словника:\", len(stoi))\n",
        "print(\"PAD:\", PAD_IDX, \"UNK:\", UNK_IDX, \"BOS:\", BOS_IDX, \"EOS:\", EOS_IDX)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBtz4NNAYUJ_",
        "outputId": "34f7a46f-ab71-4094-9f58-8399e2cc670a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 12, 1595, 12, 241, 1974, 4047, 40, 62, 368, 1099, 88, 7, 33, 4, 6976, 14, 3358, 10, 56]\n"
          ]
        }
      ],
      "source": [
        "MAX_LEN = 100   # довжина з BOS/EOS\n",
        "\n",
        "def encode_with_specials(tok_list):\n",
        "    # звичайні слова → індекси\n",
        "    ids = [stoi.get(t, UNK_IDX) for t in tok_list]\n",
        "    # ріжемо, щоб залишилось місце для BOS і EOS\n",
        "    ids = ids[:MAX_LEN - 2]\n",
        "    # додаємо спеціальні токени\n",
        "    return [BOS_IDX] + ids + [EOS_IDX]\n",
        "\n",
        "encoded_texts = [encode_with_specials(tok_list) for tok_list in tokens]\n",
        "print(encoded_texts[0][:20])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "gjWh1MuMY5Xl"
      },
      "outputs": [],
      "source": [
        "#Padding\n",
        "def pad_sequence(seq):\n",
        "    seq = seq[:MAX_LEN]\n",
        "    seq = seq + [PAD_IDX] * max(0, MAX_LEN - len(seq))\n",
        "    return torch.tensor(seq, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nie5koGTCLWz",
        "outputId": "8cd32a71-59f9-4b43-8143-4a069cc4bfb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([25000, 100]) torch.Size([25000, 100])\n",
            "tensor([   2,   12, 1595,   12,  241, 1974, 4047,   40,   62,  368])\n",
            "tensor([  12, 1595,   12,  241, 1974, 4047,   40,   62,  368, 1099])\n"
          ]
        }
      ],
      "source": [
        "#input/таргет (зсув на 1)\n",
        "def make_lm_pair(seq):\n",
        "    # seq вже містить [BOS ... EOS]\n",
        "    inp = seq[:-1]   # [BOS ... останнє слово]\n",
        "    tgt = seq[1:]    # [... EOS]\n",
        "    return inp, tgt\n",
        "\n",
        "pairs = [make_lm_pair(seq) for seq in encoded_texts]\n",
        "\n",
        "X_in  = torch.stack([pad_sequence(inp) for inp, _ in pairs])\n",
        "Y_tgt = torch.stack([pad_sequence(tgt) for _, tgt in pairs])\n",
        "\n",
        "print(X_in.shape, Y_tgt.shape)   # [N, MAX_LEN]\n",
        "print(X_in[0][:10])\n",
        "print(Y_tgt[0][:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "RfTqUNasZcy0"
      },
      "outputs": [],
      "source": [
        "#TensorDataset і DataLoader\n",
        "dataset = TensorDataset(X_in, Y_tgt) # !!!\n",
        "\n",
        "# розділяємо на train/val\n",
        "VAL_FRAC = 0.2\n",
        "val_sz = int(len(dataset) * VAL_FRAC)\n",
        "train_sz = len(dataset) - val_sz\n",
        "train_ds, val_ds = random_split(dataset, [train_sz, val_sz],\n",
        "                                generator=torch.Generator().manual_seed(42))\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True)\n",
        "val_loader   = DataLoader(val_ds, batch_size=128)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "EPTADA0WFknf"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        # pe: [max_len, d_model]\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)          # [max_len, 1]\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)\n",
        "        )\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        pe = pe.unsqueeze(0)  # [1, max_len, d_model] – для додавання до batch\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [B, T, d_model]\n",
        "        x = x + self.pe[:, : x.size(1)]\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "C-EG-v6tEpAd"
      },
      "outputs": [],
      "source": [
        "class TransformerDecoderLM(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size,\n",
        "        emb_dim,\n",
        "        pad_idx,\n",
        "        n_heads=4,\n",
        "        n_layers=2,\n",
        "        dim_feedforward=256,\n",
        "        dropout=0.1,\n",
        "        max_len=5000,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.pad_idx = pad_idx\n",
        "        self.emb_dim = emb_dim\n",
        "\n",
        "        # Слово -> вектор\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=vocab_size,\n",
        "            embedding_dim=emb_dim,\n",
        "            padding_idx=pad_idx,\n",
        "        )\n",
        "\n",
        "        # Позиційне кодування\n",
        "        self.pos_encoding = PositionalEncoding(d_model=emb_dim, max_len=max_len)\n",
        "\n",
        "        # Один шар декодера трансформера\n",
        "        decoder_layer = nn.TransformerDecoderLayer(\n",
        "            d_model=emb_dim,\n",
        "            nhead=n_heads,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "            batch_first=True,\n",
        "        )\n",
        "\n",
        "        # Стек із кількох шарів\n",
        "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=n_layers)\n",
        "\n",
        "        # Проекція у простір словника\n",
        "        self.fc_out = nn.Linear(emb_dim, vocab_size)\n",
        "\n",
        "    def _generate_square_subsequent_mask(self, T, device):\n",
        "        # каузальна маска: кожна позиція \"бачить\" тільки попередні слова\n",
        "        mask = torch.triu(torch.ones(T, T, device=device), diagonal=1)\n",
        "        mask = mask.masked_fill(mask == 1, float('-inf'))\n",
        "        return mask\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [B, T]\n",
        "        emb = self.embedding(x)           # [B, T, D]\n",
        "        emb = self.pos_encoding(emb)      # [B, T, D]\n",
        "\n",
        "        B, T, D = emb.shape\n",
        "        tgt_mask = self._generate_square_subsequent_mask(T, x.device)\n",
        "\n",
        "        out = self.decoder(\n",
        "            tgt=emb,\n",
        "            memory=emb,\n",
        "            tgt_mask=tgt_mask,\n",
        "            tgt_key_padding_mask=(x == self.pad_idx),\n",
        "            memory_key_padding_mask=(x == self.pad_idx),\n",
        "        )                                 # [B, T, D]\n",
        "\n",
        "        logits = self.fc_out(out)         # [B, T, V]\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "exzQ2xk7FDO7"
      },
      "outputs": [],
      "source": [
        "model = TransformerDecoderLM(\n",
        "    vocab_size=len(stoi),\n",
        "    emb_dim=128,\n",
        "    pad_idx=PAD_IDX,\n",
        "    n_heads=4,\n",
        "    n_layers=2,\n",
        "    dim_feedforward=256,\n",
        "    dropout=0.1,\n",
        "    max_len=MAX_LEN,\n",
        ").to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "nlbZR467EhCg"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "EjhcSg5DKDx6"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)      # x,y: [B, T]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(x)                      # [B, T, V]\n",
        "\n",
        "        B, T, V = logits.shape\n",
        "        loss = criterion(\n",
        "            logits.view(B*T, V),\n",
        "            y.view(B*T)\n",
        "        )\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * B\n",
        "\n",
        "    return running_loss / len(loader.dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "bZzmCQEHKAWq"
      },
      "outputs": [],
      "source": [
        "def evaluate_one_epoch(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            logits = model(x)\n",
        "\n",
        "            B, T, V = logits.shape\n",
        "            loss = criterion(\n",
        "                logits.view(B*T, V),\n",
        "                y.view(B*T)\n",
        "            )\n",
        "\n",
        "            running_loss += loss.item() * B\n",
        "\n",
        "    return running_loss / len(loader.dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5crVj8tAi59",
        "outputId": "20d29c2e-b7a8-44c1-90d8-f16a49efd52f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:6041: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30 - loss: 6.6926 - val_loss: 5.8310\n",
            "Epoch 2/30 - loss: 5.1204 - val_loss: 4.0801\n",
            "Epoch 3/30 - loss: 3.3228 - val_loss: 2.2321\n",
            "Epoch 4/30 - loss: 1.8240 - val_loss: 1.1870\n",
            "Epoch 5/30 - loss: 1.0335 - val_loss: 0.7400\n",
            "Epoch 6/30 - loss: 0.6355 - val_loss: 0.5175\n",
            "Epoch 7/30 - loss: 0.4188 - val_loss: 0.3860\n",
            "Epoch 8/30 - loss: 0.2868 - val_loss: 0.3088\n",
            "Epoch 9/30 - loss: 0.2032 - val_loss: 0.2638\n",
            "Epoch 10/30 - loss: 0.1491 - val_loss: 0.2285\n",
            "Epoch 11/30 - loss: 0.1098 - val_loss: 0.2099\n",
            "Epoch 12/30 - loss: 0.0834 - val_loss: 0.1904\n",
            "Epoch 13/30 - loss: 0.0642 - val_loss: 0.1817\n",
            "Epoch 14/30 - loss: 0.0503 - val_loss: 0.1681\n",
            "Epoch 15/30 - loss: 0.0405 - val_loss: 0.1646\n",
            "Epoch 16/30 - loss: 0.0329 - val_loss: 0.1562\n",
            "Epoch 17/30 - loss: 0.0272 - val_loss: 0.1496\n",
            "Epoch 18/30 - loss: 0.0231 - val_loss: 0.1461\n",
            "Epoch 19/30 - loss: 0.0196 - val_loss: 0.1441\n",
            "Epoch 20/30 - loss: 0.0171 - val_loss: 0.1390\n",
            "Epoch 21/30 - loss: 0.0152 - val_loss: 0.1387\n",
            "Epoch 22/30 - loss: 0.0137 - val_loss: 0.1348\n",
            "Epoch 23/30 - loss: 0.0126 - val_loss: 0.1324\n",
            "Epoch 24/30 - loss: 0.0110 - val_loss: 0.1323\n",
            "Epoch 25/30 - loss: 0.0104 - val_loss: 0.1263\n",
            "Epoch 26/30 - loss: 0.0095 - val_loss: 0.1258\n",
            "Epoch 27/30 - loss: 0.0088 - val_loss: 0.1233\n",
            "Epoch 28/30 - loss: 0.0084 - val_loss: 0.1225\n",
            "Epoch 29/30 - loss: 0.0081 - val_loss: 0.1206\n",
            "Epoch 30/30 - loss: 0.0075 - val_loss: 0.1207\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 30\n",
        "\n",
        "for ep in range(1, EPOCHS+1):\n",
        "    train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
        "    val_loss = evaluate_one_epoch(model, val_loader, criterion, device)\n",
        "    print(f\"Epoch {ep}/{EPOCHS} - loss: {train_loss:.4f} - val_loss: {val_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "1KsjWmX6I-wt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4c7d328-08ba-4690-8850-a763709ca7d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this movie was was slayer wore wore mraovich daisy was was was kissing\n"
          ]
        }
      ],
      "source": [
        "itos = {i: w for w, i in stoi.items()}\n",
        "\n",
        "def generate(model, start_text, max_new_tokens=20):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # токенізуємо й чистимо\n",
        "        tokens = word_tokenize(clean_text(start_text))\n",
        "        ids = [BOS_IDX] + [stoi.get(t, UNK_IDX) for t in tokens]\n",
        "        ids = ids[:MAX_LEN-1]  # щоб був запас для EOS\n",
        "\n",
        "        seq = torch.tensor([ids], dtype=torch.long, device=device)\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            logits = model(seq)              # [1, T, V]\n",
        "            next_logits = logits[0, -1]      # остання позиція\n",
        "\n",
        "            temperature = 2.0\n",
        "            probs = torch.softmax(next_logits / temperature, dim=-1)\n",
        "\n",
        "\n",
        "            next_idx = torch.multinomial(probs, num_samples=1).item()\n",
        "\n",
        "            seq = torch.cat(\n",
        "                [seq, torch.tensor([[next_idx]], device=device)],\n",
        "                dim=1\n",
        "            )\n",
        "\n",
        "            if next_idx == EOS_IDX:\n",
        "                break\n",
        "\n",
        "        # перетворюємо назад у слова, ігноруємо PAD/BOS/EOS\n",
        "        ids = seq[0].cpu().tolist()\n",
        "        words = []\n",
        "        for idx in ids:\n",
        "            if idx in (PAD_IDX, BOS_IDX, EOS_IDX):\n",
        "                continue\n",
        "            words.append(itos.get(idx, \"<UNK>\"))\n",
        "        return \" \".join(words)\n",
        "\n",
        "\n",
        "print(generate(model, \"this movie was\", max_new_tokens=10))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyOKi+7Zmc8dBVXjd8AerrB5",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}