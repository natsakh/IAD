{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPXIhUPgG+NChE4perT05gU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/natsakh/IAD/blob/main/Pr_7/7_4_Encoder_Decoder_MT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Machine Translation"
      ],
      "metadata": {
        "id": "ZTF_MMWNfzY3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://keras.io/examples/nlp/neural_machine_translation_with_transformer/"
      ],
      "metadata": {
        "id": "830qfGZa7nwV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
        "from collections import Counter\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import random\n",
        "import math\n",
        "\n",
        "torch.manual_seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRhYEceEX9gy",
        "outputId": "80776303-9bb1-47c0-a2a7-f34af56b7f69"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tab-delimited Bilingual Sentence Pairs\n",
        "https://www.manythings.org/anki/\n"
      ],
      "metadata": {
        "id": "2VWhLfpmYLRl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ksa7zcNaX4B1",
        "outputId": "7fa45bd3-dcd7-4bc6-8247-b77c380d8c78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-11-21 17:54:08--  https://www.manythings.org/anki/spa-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 173.254.30.110\n",
            "Connecting to www.manythings.org (www.manythings.org)|173.254.30.110|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5466500 (5.2M) [application/zip]\n",
            "Saving to: ‘spa-eng.zip’\n",
            "\n",
            "spa-eng.zip         100%[===================>]   5.21M  3.12MB/s    in 1.7s    \n",
            "\n",
            "2025-11-21 17:54:10 (3.12 MB/s) - ‘spa-eng.zip’ saved [5466500/5466500]\n",
            "\n",
            "Archive:  spa-eng.zip\n",
            "  inflating: _about.txt              \n",
            "  inflating: spa.txt                 \n"
          ]
        }
      ],
      "source": [
        "!wget https://www.manythings.org/anki/spa-eng.zip\n",
        "!unzip -o spa-eng.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lines = open(\"spa.txt\", encoding=\"utf-8\").read().split(\"\\n\")\n",
        "print(lines[0])\n",
        "print()\n",
        "\n",
        "pairs = []\n",
        "for line in lines:\n",
        "    parts = line.split(\"\\t\")\n",
        "    if len(parts) < 2:\n",
        "        continue\n",
        "    eng = parts[0].strip()\n",
        "    spa = parts[1].strip()\n",
        "    pairs.append((eng, spa))\n",
        "\n",
        "print(pairs[0])\n",
        "print(len(pairs))\n",
        "print()\n",
        "\n",
        "for _ in range(5):\n",
        "    print(random.choice(pairs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tlqKN7YPYYXv",
        "outputId": "05066148-f4e0-45b5-a6c3-63c9a7f1dbb4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Go.\tVe.\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #4986655 (cueyayotl)\n",
            "\n",
            "('Go.', 'Ve.')\n",
            "142928\n",
            "\n",
            "('Do you have a violin?', '¿Tienes un violín?')\n",
            "('This is my CD.', 'Es mi CD.')\n",
            "('She made him a simple dinner.', 'Ella le hizo una cena sencilla.')\n",
            "('Are you guys having any fun?', '¿Os estáis divirtiendo?')\n",
            "('We swam until it got dark.', 'Nadamos hasta que oscureció.')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_VOCAB = 15000\n",
        "specials = [\"[PAD]\", \"[UNK]\", \"[BOS]\", \"[EOS]\"]\n",
        "\n",
        "\n",
        "# Токенізатор (простий)\n",
        "def tokenize(text):\n",
        "    return text.lower().strip().split()"
      ],
      "metadata": {
        "id": "b363R38N72E0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Будуємо словник\n",
        "def build_vocab(sentences):\n",
        "    counter = Counter()\n",
        "\n",
        "    for s in sentences:\n",
        "        counter.update(tokenize(s))\n",
        "\n",
        "    most_common = counter.most_common(MAX_VOCAB - len(specials))\n",
        "    itos = specials + [w for w, _ in most_common]\n",
        "    stoi = {w: i for i, w in enumerate(itos)}\n",
        "\n",
        "    return stoi, itos\n"
      ],
      "metadata": {
        "id": "KtdU17jvALea"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Розділяємо дані\n",
        "eng_sentences = [p[0] for p in pairs]\n",
        "spa_sentences = [p[1] for p in pairs]\n",
        "\n",
        "# Створюємо два словники\n",
        "eng_stoi, eng_itos = build_vocab(eng_sentences)\n",
        "spa_stoi, spa_itos = build_vocab(spa_sentences)\n",
        "\n",
        "PAD_IDX = eng_stoi[\"[PAD]\"]    # однакові індекси у двох мовах — добре\n",
        "UNK_IDX = eng_stoi[\"[UNK]\"]\n",
        "BOS_IDX = eng_stoi[\"[BOS]\"]\n",
        "EOS_IDX = eng_stoi[\"[EOS]\"]\n",
        "\n",
        "print(\"ENG vocab size:\", len(eng_stoi))\n",
        "print(\"SPA vocab size:\", len(spa_stoi))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCht8HXUAVJG",
        "outputId": "50b91aa6-2fd3-4cb0-dc4b-64cf828b3835"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ENG vocab size: 15000\n",
            "SPA vocab size: 15000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN_SRC = 20   # довжина англ речення\n",
        "MAX_LEN_TGT = 20   # довжина іспанського (включно з BOS/EOS)\n",
        "\n",
        "def encode(text, stoi, add_specials=False):\n",
        "    tokens = tokenize(text)\n",
        "    ids = []\n",
        "\n",
        "    if add_specials:\n",
        "        ids.append(BOS_IDX)\n",
        "\n",
        "    for t in tokens:\n",
        "        ids.append(stoi.get(t, UNK_IDX))\n",
        "\n",
        "    if add_specials:\n",
        "        ids.append(EOS_IDX)\n",
        "\n",
        "    return ids"
      ],
      "metadata": {
        "id": "nRxs7JhmA3MU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_pair(eng, spa):\n",
        "    # encoder input: англ без BOS/EOS\n",
        "    src = encode(eng, eng_stoi, add_specials=False)\n",
        "\n",
        "    # повна цільова послідовність: [BOS ... EOS]\n",
        "    tgt_full = encode(spa, spa_stoi, add_specials=True)\n",
        "\n",
        "    # decoder_inputs: без останнього токена\n",
        "    tgt_in = tgt_full[:-1]\n",
        "\n",
        "    # targets: без першого токена (зсунуті)\n",
        "    tgt_out = tgt_full[1:]\n",
        "\n",
        "    # паддинг\n",
        "    src = src[:MAX_LEN_SRC] + [PAD_IDX] * (MAX_LEN_SRC - len(src))\n",
        "    tgt_in = tgt_in[:MAX_LEN_TGT] + [PAD_IDX] * (MAX_LEN_TGT - len(tgt_in))\n",
        "    tgt_out = tgt_out[:MAX_LEN_TGT] + [PAD_IDX] * (MAX_LEN_TGT - len(tgt_out))\n",
        "\n",
        "    return src, tgt_in, tgt_out\n"
      ],
      "metadata": {
        "id": "R42WO3hrBUWv"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, pairs):\n",
        "        self.pairs = pairs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        eng, spa = self.pairs[idx]\n",
        "        src, tgt_in, tgt_out = prepare_pair(eng, spa)\n",
        "\n",
        "        return (\n",
        "            torch.tensor(src, dtype=torch.long),\n",
        "            torch.tensor(tgt_in, dtype=torch.long),\n",
        "            torch.tensor(tgt_out, dtype=torch.long),\n",
        "        )"
      ],
      "metadata": {
        "id": "PbIug3T5BoJD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# зробимо train/val/test спліт\n",
        "random.shuffle(pairs)\n",
        "\n",
        "total = len(pairs)\n",
        "train_size = int(0.95 * total)\n",
        "\n",
        "\n",
        "train_pairs = pairs[:train_size]\n",
        "val_pairs   = pairs[train_size : ]\n",
        "\n",
        "\n",
        "print(\"Train:\", len(train_pairs))\n",
        "print(\"Val:\", len(val_pairs))\n",
        "\n",
        "print(\"Total:\", len(train_pairs) + len(val_pairs))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34-WuoEXB7TS",
        "outputId": "6e67eacf-ddb2-4bfc-c0ff-ae44c5bb45e9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 135781\n",
            "Val: 7147\n",
            "Total: 142928\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = TranslationDataset(train_pairs)\n",
        "val_ds   = TranslationDataset(val_pairs)\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE)\n"
      ],
      "metadata": {
        "id": "Q5-C2cNODg4d"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src, tgt_in, tgt_out = next(iter(train_loader))\n",
        "print(\"src:\", src.shape)       # [B, MAX_LEN_SRC]\n",
        "print(\"tgt_in:\", tgt_in.shape) # [B, MAX_LEN_TGT]\n",
        "print(\"tgt_out:\", tgt_out.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUUq0Uu1DmRr",
        "outputId": "f56a948c-c774-4d09-c7eb-08d7418ede77"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "src: torch.Size([64, 20])\n",
            "tgt_in: torch.Size([64, 20])\n",
            "tgt_out: torch.Size([64, 20])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_padding_mask(x, pad_idx=PAD_IDX):\n",
        "    # x: [B, T]\n",
        "    # True там, де пади (так хоче nn.Transformer)\n",
        "    return (x == pad_idx)\n",
        "\n",
        "def generate_subsequent_mask(size, device):\n",
        "    # True = заборонено\n",
        "    mask = torch.triu(torch.ones(size, size, dtype=torch.bool, device=device), diagonal=1)\n",
        "    return mask\n",
        "\n",
        "class TokenPositionalEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, max_len=50, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.token_emb = nn.Embedding(vocab_size, d_model, padding_idx=PAD_IDX)\n",
        "        self.pos_emb = nn.Embedding(max_len, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [B, T]\n",
        "        B, T = x.shape\n",
        "        pos = torch.arange(T, device=x.device).unsqueeze(0).expand(B, T)\n",
        "        x = self.token_emb(x) + self.pos_emb(pos)\n",
        "        return self.dropout(x)"
      ],
      "metadata": {
        "id": "xn4ilsf_EDfc"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2SeqTransformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 src_vocab_size,\n",
        "                 tgt_vocab_size,\n",
        "                 d_model=64,\n",
        "                 n_heads=4,\n",
        "                 num_layers=2,\n",
        "                 d_ff=128,\n",
        "                 max_len_src=20,\n",
        "                 max_len_tgt=20,\n",
        "                 dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.src_emb = TokenPositionalEmbedding(src_vocab_size, d_model, max_len_src, dropout)\n",
        "        self.tgt_emb = TokenPositionalEmbedding(tgt_vocab_size, d_model, max_len_tgt, dropout)\n",
        "\n",
        "        enc_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=n_heads,\n",
        "            dim_feedforward=d_ff,\n",
        "            dropout=dropout,\n",
        "            batch_first=True,\n",
        "        )\n",
        "        dec_layer = nn.TransformerDecoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=n_heads,\n",
        "            dim_feedforward=d_ff,\n",
        "            dropout=dropout,\n",
        "            batch_first=True,\n",
        "        )\n",
        "\n",
        "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n",
        "        self.decoder = nn.TransformerDecoder(dec_layer, num_layers=num_layers)\n",
        "\n",
        "        self.output_proj = nn.Linear(d_model, tgt_vocab_size)\n",
        "\n",
        "    def forward(self, src, tgt_in):\n",
        "        \"\"\"\n",
        "        src: [B, S]\n",
        "        tgt_in: [B, T]\n",
        "        \"\"\"\n",
        "        src_key_padding_mask = make_padding_mask(src)      # [B, S]\n",
        "        tgt_key_padding_mask = make_padding_mask(tgt_in)   # [B, T]\n",
        "\n",
        "        # embeddings\n",
        "        enc_in = self.src_emb(src)     # [B, S, D]\n",
        "        dec_in = self.tgt_emb(tgt_in)  # [B, T, D]\n",
        "\n",
        "        # encoder\n",
        "        memory = self.encoder(\n",
        "            enc_in,\n",
        "            src_key_padding_mask=src_key_padding_mask\n",
        "        )  # [B, S, D]\n",
        "\n",
        "        # декодерська causal mask\n",
        "        T = tgt_in.size(1)\n",
        "        tgt_mask = generate_subsequent_mask(T, device=tgt_in.device)  # [T,T]\n",
        "\n",
        "        dec_out = self.decoder(\n",
        "            dec_in,\n",
        "            memory,\n",
        "            tgt_mask=tgt_mask,\n",
        "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
        "            memory_key_padding_mask=src_key_padding_mask,\n",
        "        )  # [B, T, D]\n",
        "\n",
        "        logits = self.output_proj(dec_out)  # [B, T, V_tgt]\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "wBloY9nqD-6B"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src_vocab_size = len(eng_stoi)\n",
        "tgt_vocab_size = len(spa_stoi)\n",
        "\n",
        "model = Seq2SeqTransformer(\n",
        "    src_vocab_size=src_vocab_size,\n",
        "    tgt_vocab_size=tgt_vocab_size,\n",
        "    d_model=64,\n",
        "    n_heads=4,\n",
        "    num_layers=4,\n",
        "    d_ff=128,\n",
        "    max_len_src=MAX_LEN_SRC,\n",
        "    max_len_tgt=MAX_LEN_TGT,\n",
        ").to(device)"
      ],
      "metadata": {
        "id": "NtxhSeXiEhFk"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(\"Trainable parameters:\", count_parameters(model))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npVHQz6FKuff",
        "outputId": "dd029506-fce2-406e-88f4-64db5b23c9c8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainable parameters: 3232408\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for name, param in model.named_parameters():\n",
        "#     print(f\"{name:40s}  {param.numel()}\")\n"
      ],
      "metadata": {
        "id": "BleH3WAMKxdp"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "FXk4J-QBEoek"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    n_tokens = 0\n",
        "\n",
        "    for src, tgt_in, tgt_out in loader:\n",
        "        src = src.to(device)\n",
        "        tgt_in = tgt_in.to(device)\n",
        "        tgt_out = tgt_out.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        logits = model(src, tgt_in)  # [B, T, V]\n",
        "        B, T, V = logits.shape\n",
        "\n",
        "        loss = criterion(\n",
        "            logits.view(B*T, V),\n",
        "            tgt_out.view(B*T)\n",
        "        )\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * (tgt_out != PAD_IDX).sum().item()\n",
        "        n_tokens += (tgt_out != PAD_IDX).sum().item()\n",
        "\n",
        "    return total_loss / n_tokens\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    n_tokens = 0\n",
        "\n",
        "    for src, tgt_in, tgt_out in loader:\n",
        "        src = src.to(device)\n",
        "        tgt_in = tgt_in.to(device)\n",
        "        tgt_out = tgt_out.to(device)\n",
        "\n",
        "        logits = model(src, tgt_in)\n",
        "        B, T, V = logits.shape\n",
        "\n",
        "        loss = criterion(\n",
        "            logits.view(B*T, V),\n",
        "            tgt_out.view(B*T)\n",
        "        )\n",
        "\n",
        "        total_loss += loss.item() * (tgt_out != PAD_IDX).sum().item()\n",
        "        n_tokens += (tgt_out != PAD_IDX).sum().item()\n",
        "\n",
        "    return total_loss / n_tokens\n"
      ],
      "metadata": {
        "id": "RN9srZVKEu9v"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "for ep in range(1, EPOCHS+1):\n",
        "    train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
        "    val_loss = evaluate(model, val_loader, criterion, device)\n",
        "    print(f\"Epoch {ep}/{EPOCHS} - train loss/token: {train_loss:.4f}  val loss/token: {val_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CKQBVifE3HX",
        "outputId": "477122ee-c6fd-4f66-a1bf-a03773b6efca"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:515: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)\n",
            "  output = torch._nested_tensor_from_mask(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 - train loss/token: 4.4845  val loss/token: 3.2712\n",
            "Epoch 2/10 - train loss/token: 2.9972  val loss/token: 2.4903\n",
            "Epoch 3/10 - train loss/token: 2.4028  val loss/token: 2.1366\n",
            "Epoch 4/10 - train loss/token: 2.0817  val loss/token: 1.9449\n",
            "Epoch 5/10 - train loss/token: 1.8806  val loss/token: 1.8200\n",
            "Epoch 6/10 - train loss/token: 1.7396  val loss/token: 1.7385\n",
            "Epoch 7/10 - train loss/token: 1.6341  val loss/token: 1.6877\n",
            "Epoch 8/10 - train loss/token: 1.5490  val loss/token: 1.6349\n",
            "Epoch 9/10 - train loss/token: 1.4760  val loss/token: 1.5971\n",
            "Epoch 10/10 - train loss/token: 1.4184  val loss/token: 1.5630\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_src_sentence(text, stoi, max_len):\n",
        "    tokens = tokenize(text)\n",
        "    ids = [eng_stoi.get(t, UNK_IDX) for t in tokens]\n",
        "    ids = ids[:max_len] + [PAD_IDX] * (max_len - len(ids))\n",
        "    return torch.tensor(ids, dtype=torch.long).unsqueeze(0)  # [1, S]\n",
        "\n",
        "\n",
        "def decode_tgt_ids(ids, itos):\n",
        "    # ids: список індексів (наприклад, з BOS/EOS/PAD)\n",
        "    tokens = []\n",
        "    for i in ids:\n",
        "        if i in (PAD_IDX, BOS_IDX, EOS_IDX):\n",
        "            continue\n",
        "        tokens.append(itos[i])\n",
        "    return \" \".join(tokens)\n"
      ],
      "metadata": {
        "id": "UjmDgnwRJKUO"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def translate(model, sentence, max_len=MAX_LEN_TGT):\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    # 1) кодуємо англійське речення\n",
        "    src = encode_src_sentence(sentence, eng_stoi, MAX_LEN_SRC).to(device)  # [1, S]\n",
        "    src_key_padding_mask = make_padding_mask(src)  # [1, S]\n",
        "\n",
        "    # 2) пропускаємо через encoder\n",
        "    enc_in = model.src_emb(src)  # [1, S, D]\n",
        "    memory = model.encoder(enc_in, src_key_padding_mask=src_key_padding_mask)  # [1, S, D]\n",
        "\n",
        "    # 3) старт декодера з BOS\n",
        "    tgt_ids = [BOS_IDX]\n",
        "    for _ in range(max_len):\n",
        "        tgt_tensor = torch.tensor(tgt_ids, dtype=torch.long, device=device).unsqueeze(0)  # [1, T]\n",
        "        tgt_key_padding_mask = make_padding_mask(tgt_tensor)\n",
        "\n",
        "        T = tgt_tensor.size(1)\n",
        "        tgt_mask = generate_subsequent_mask(T, device=device)  # [T, T]\n",
        "\n",
        "        dec_in = model.tgt_emb(tgt_tensor)  # [1, T, D]\n",
        "        dec_out = model.decoder(\n",
        "            dec_in,\n",
        "            memory,\n",
        "            tgt_mask=tgt_mask,\n",
        "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
        "            memory_key_padding_mask=src_key_padding_mask,\n",
        "        )  # [1, T, D]\n",
        "\n",
        "        logits = model.output_proj(dec_out[:, -1, :])  # [1, V]\n",
        "        next_id = logits.argmax(dim=-1).item()\n",
        "\n",
        "        tgt_ids.append(next_id)\n",
        "\n",
        "        if next_id == EOS_IDX:\n",
        "            break\n",
        "\n",
        "    # 4) декодуємо у текст\n",
        "    translation = decode_tgt_ids(tgt_ids, spa_itos)\n",
        "    return translation\n"
      ],
      "metadata": {
        "id": "ZTmxIgA_JMhs"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "examples = [\n",
        "    \"Go.\",\n",
        "    \"Hi!\",\n",
        "    \"Good morning.\",\n",
        "    \"Good night.\",\n",
        "    \"I am fine.\",\n",
        "    \"See you later.\",\n",
        "    \"I forgot my keys at home.\",\n",
        "    \"We are looking for a new apartment.\",\n",
        "    \"She didn’t come because she was sick.\",\n",
        "]\n",
        "\n",
        "for s in examples:\n",
        "    print(\"EN:\", s)\n",
        "    print(\"ES:\", translate(model, s))\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ThHxwX_KJPS6",
        "outputId": "1dbfa4ac-4de7-4c48-b822-a551924d4473"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EN: Go.\n",
            "ES: vete.\n",
            "\n",
            "EN: Hi!\n",
            "ES: ¡hola!\n",
            "\n",
            "EN: Good morning.\n",
            "ES: buenas mañanas.\n",
            "\n",
            "EN: Good night.\n",
            "ES: buenas noches la noche.\n",
            "\n",
            "EN: I am fine.\n",
            "ES: estoy bien.\n",
            "\n",
            "EN: See you later.\n",
            "ES: ¡hasta luego.\n",
            "\n",
            "EN: I forgot my keys at home.\n",
            "ES: olvidé mis llaves en casa.\n",
            "\n",
            "EN: We are looking for a new apartment.\n",
            "ES: estamos buscando un nuevo apartamento.\n",
            "\n",
            "EN: She didn’t come because she was sick.\n",
            "ES: ella se adaptó porque estaba enfermo.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}