{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNcGFFBP4EHojyH7DYmVpEt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/natsakh/IAD/blob/main/Pr_7/7_2_Encoder_IMDB_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import re, string\n",
        "from collections import Counter\n",
        "\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
        "import random\n",
        "\n",
        "torch.manual_seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5c3tJeq44UtG",
        "outputId": "40c31d5e-8748-4634-96ce-85758c5d7ab8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hugging Face Datasets — бібліотека для завантаження датасетів.\n",
        "#У Google Colab вона вже попередньо встановлена\n",
        "# Якщо ви працюєте локально — попередньо виконайте:\n",
        "#     pip install datasets\n",
        "from datasets import load_dataset\n",
        "#https://huggingface.co/datasets"
      ],
      "metadata": {
        "id": "LuCCdUwkATes"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = load_dataset(\"imdb\")\n",
        "#ds[\"train\"], ds[\"test\"]\n",
        "#список словників — кожен елемент має ключі 'text' і 'label'"
      ],
      "metadata": {
        "id": "dxkvdrcw_vWb"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dymBEXUDxGU",
        "outputId": "5de8b5a6-c430-48bf-a55d-bd887eb04a4e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 25000\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 25000\n",
            "    })\n",
            "    unsupervised: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 50000\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example = ds[\"train\"][0]\n",
        "print('text: ', example['text'])\n",
        "print('label: ', example['label'])\n",
        "#label = 1 → позитивний відгук, 0 → негативний."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8s7EOUoFUwI",
        "outputId": "583c8fd3-398b-46f7-d9d0-5238dd770094"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text:  I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it's not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn't have much of a plot.\n",
            "label:  0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    text = text.lower()                                       # до нижнього регістру\n",
        "    text = re.sub(r\"@\\S+\", \" \", text)                         # прибрати згадки @user\n",
        "    text = re.sub(r\"http\\S+\", \" \", text)                      # прибрати посилання\n",
        "    text = re.sub(r\"<.*?>\", \" \", text)                        # прибрати HTML-теги\n",
        "    text = re.sub(r\"[^a-z\\s]\", \" \", text)                     # залишити лише букви (прибрати цифри, спецсимволи)\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)  # прибрати пунктуацію\n",
        "    text = re.sub(r\"\\s+\", \" \", text)                          # замінити багато пробілів на один\n",
        "    return text"
      ],
      "metadata": {
        "id": "KI9DmAi0JvDt"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_texts = [clean_text(ex[\"text\"]) for ex in ds[\"train\"]]\n",
        "tokens = [word_tokenize(t) for t in cleaned_texts]"
      ],
      "metadata": {
        "id": "tJwNUD7WJ8p1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "counter = Counter()\n",
        "for tok_list in tokens:\n",
        "    counter.update(tok_list)"
      ],
      "metadata": {
        "id": "OTnFpSWP5O_D"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# кількість унікальних слів\n",
        "vocab_size = len(counter)\n",
        "\n",
        "print(f\"Кількість унікальних слів у train-наборі: {vocab_size:,}\")\n",
        "\n",
        "MAX_VOCAB = 20_000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "301g3yaR5pgK",
        "outputId": "7812b4b0-7aa9-408c-bf84-aca813bb14a3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Кількість унікальних слів у train-наборі: 73,206\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# створюємо списки перетворень, створюємо словник\n",
        "specials = [\"[PAD]\", \"[UNK]\"]\n",
        "most_common = counter.most_common(MAX_VOCAB - len(specials))\n",
        "itos = specials + [w for w, _ in most_common]       # index → string\n",
        "stoi = {w: i for i, w in enumerate(itos)}           # string → index\n",
        "\n",
        "PAD_IDX = stoi[\"[PAD]\"]\n",
        "UNK_IDX = stoi[\"[UNK]\"]\n",
        "\n",
        "print(\"Розмір словника:\", len(stoi))\n",
        "print(\"Приклад індексу:\", stoi.get(\"movie\", UNK_IDX))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hhmoio6xM03x",
        "outputId": "2668d0da-31db-4095-d33c-d4f6ea40d000"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Розмір словника: 20000\n",
            "Приклад індексу: 18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#кодуємо тексти у послідовності індексів\n",
        "#для кожного токена беремо його індекс зі словника stoi,\n",
        "#якщо слова немає — повертаємо індекс UNK_IDX.\n",
        "def encode(tokens):\n",
        "    return [stoi.get(t, UNK_IDX) for t in tokens]\n",
        "\n",
        "encoded_texts = [encode(tok_list) for tok_list in tokens]\n",
        "print(encoded_texts[0][:20])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBtz4NNAYUJ_",
        "outputId": "18e1fdf4-5069-4bc0-8304-6a6723bdaf6a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[10, 1593, 10, 239, 1972, 4045, 38, 60, 366, 1097, 86, 5, 31, 2, 6974, 12, 3356, 8, 54, 8]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Padding (вирівнюємо довжину)\n",
        "#Нейронна мережа очікує тензори однакової довжини,\n",
        "#тому короткі тексти “доповнюємо” <pad>, а надто довгі — обрізаємо\n",
        "\n",
        "MAX_LEN = 100   # довжина послідовності\n",
        "def pad_sequence(seq):\n",
        "    seq = seq[:MAX_LEN] + [PAD_IDX] * max(0, MAX_LEN - len(seq))\n",
        "    return torch.tensor(seq, dtype=torch.long)\n",
        "\n",
        "X = torch.stack([pad_sequence(seq) for seq in encoded_texts])\n",
        "print(X.shape)   # [N, MAX_LEN]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjWh1MuMY5Xl",
        "outputId": "7aa6842c-9ab8-4bf6-b018-6fcfdbc9c634"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([25000, 100])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# мітки (labels)\n",
        "y = torch.tensor([ex[\"label\"] for ex in ds[\"train\"]], dtype=torch.float32)\n",
        "print(y.shape)   # [N]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JdVlkiH-ZSho",
        "outputId": "284b7d33-417b-4127-b091-81972759d337"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([25000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TensorDataset і DataLoader\n",
        "dataset = TensorDataset(X, y)\n",
        "\n",
        "# розділяємо на train/val\n",
        "VAL_FRAC = 0.2\n",
        "val_sz = int(len(dataset) * VAL_FRAC)\n",
        "train_sz = len(dataset) - val_sz\n",
        "train_ds, val_ds = random_split(dataset, [train_sz, val_sz],\n",
        "                                generator=torch.Generator().manual_seed(42))\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True)\n",
        "val_loader   = DataLoader(val_ds, batch_size=128)\n"
      ],
      "metadata": {
        "id": "RfTqUNasZcy0"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# використовуємо ті самі clean_text, токенайзер/word_tokenize, stoi, PAD_IDX, MAX_LEN\n",
        "\n",
        "# Підготовка X_test, y_test\n",
        "cleaned_test = [clean_text(ex[\"text\"]) for ex in ds[\"test\"]]\n",
        "tokens_test  = [word_tokenize(t) for t in cleaned_test]\n",
        "encoded_test = [[stoi.get(tok, UNK_IDX) for tok in toks] for toks in tokens_test]\n",
        "\n",
        "def pad_sequence(seq):\n",
        "    seq = seq[:MAX_LEN] + [PAD_IDX] * max(0, MAX_LEN - len(seq))\n",
        "    return torch.tensor(seq, dtype=torch.long)\n",
        "\n",
        "X_test = torch.stack([pad_sequence(seq) for seq in encoded_test])\n",
        "y_test = torch.tensor([ex[\"label\"] for ex in ds[\"test\"]], dtype=torch.float32)\n",
        "\n",
        "# 2) TensorDataset + DataLoader\n",
        "test_ds = TensorDataset(X_test, y_test)\n",
        "test_loader = DataLoader(test_ds, batch_size=128, shuffle=False)  # на тесті shuffle=False\n"
      ],
      "metadata": {
        "id": "FYz9uqj0hgLC"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(x)\n",
        "        loss = criterion(out, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "    train_loss /= len(loader)\n",
        "\n",
        "    return train_loss\n"
      ],
      "metadata": {
        "id": "m5crVj8tAi59"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_one_epoch(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    valid_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for x, y in loader:\n",
        "          x, y = x.to(device), y.to(device)\n",
        "          out = model(x)\n",
        "          loss = criterion(out, y)\n",
        "          valid_loss += loss.item()\n",
        "    valid_loss /= len(loader)\n",
        "\n",
        "    return valid_loss"
      ],
      "metadata": {
        "id": "szuIUjuXAonL"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        # pe: [max_len, d_model]\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)          # [max_len, 1]\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)\n",
        "        )\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        pe = pe.unsqueeze(0)  # [1, max_len, d_model] – для додавання до batch\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [B, T, d_model]\n",
        "        x = x + self.pe[:, : x.size(1)]\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "1KsjWmX6I-wt"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder_IMDB(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size,\n",
        "        emb_dim,\n",
        "        pad_idx,\n",
        "        n_heads=4,\n",
        "        n_layers=2,\n",
        "        dim_feedforward=256,\n",
        "        dropout=0.1,\n",
        "        max_len=5000,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.pad_idx = pad_idx\n",
        "        self.emb_dim = emb_dim\n",
        "\n",
        "        # Слово → вектор\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=vocab_size,\n",
        "            embedding_dim=emb_dim,\n",
        "            padding_idx=pad_idx\n",
        "        )\n",
        "\n",
        "        # Позиційні кодування\n",
        "        self.pos_encoding = PositionalEncoding(d_model=emb_dim, max_len=max_len)\n",
        "\n",
        "        # Один шар енкодера трансформера\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=emb_dim,\n",
        "            nhead=n_heads,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "            batch_first=True,   # очікуємо [B, T, D]\n",
        "        )\n",
        "        # Стек із кількох шарів\n",
        "        self.encoder = nn.TransformerEncoder(\n",
        "            encoder_layer,\n",
        "            num_layers=n_layers\n",
        "        )\n",
        "\n",
        "        # Класифікаційна \"голова\"\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(emb_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [B, T] – індекси слів\n",
        "\n",
        "        # Створюємо маску паддінгу: True там, де PAD\n",
        "        src_key_padding_mask = (x == self.pad_idx)   # [B, T], bool\n",
        "\n",
        "        # Ембедінги + масштабуємо\n",
        "        x = self.embedding(x) * math.sqrt(self.emb_dim)  # [B, T, D]\n",
        "\n",
        "        # Додаємо позиційні кодування\n",
        "        x = self.pos_encoding(x)  # [B, T, D]\n",
        "\n",
        "        # Проганяємо через TransformerEncoder\n",
        "        # src_key_padding_mask: True = token буде ігноруватись\n",
        "        enc_out = self.encoder(\n",
        "            x,\n",
        "            src_key_padding_mask=src_key_padding_mask\n",
        "        )  # [B, T, D]\n",
        "\n",
        "        pooled = enc_out.mean(dim=1)   # [B, D]\n",
        "\n",
        "        logits = self.fc(self.dropout(pooled))   # [B, 1]\n",
        "        return logits.squeeze(1)                 # [B]\n",
        "\n"
      ],
      "metadata": {
        "id": "K5ISqq78znT6"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(stoi)\n",
        "emb_dim = 100\n",
        "hidden_dim = 64\n",
        "pad_idx = stoi[\"[PAD]\"]\n",
        "\n",
        "model_transformer = TransformerEncoder_IMDB(\n",
        "    vocab_size=vocab_size,\n",
        "    emb_dim=emb_dim,\n",
        "    pad_idx=pad_idx,\n",
        "    n_heads=4,          # 100 % 4 == 0 – ок\n",
        "    n_layers=2,\n",
        "    dim_feedforward=256,\n",
        "    dropout=0.1,\n",
        "    max_len=MAX_LEN     #  MAX_LEN=100\n",
        ").to(device)\n"
      ],
      "metadata": {
        "id": "P2Tr1vNVdAi2"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_params = sum(p.numel() for p in model_transformer.parameters() if p.requires_grad)\n",
        "print(\"Trainable params:\", num_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-17vs4ONHDb",
        "outputId": "d300d493-39e6-43ce-eab4-33dbf9b6658a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainable params: 2184813\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model_transformer.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "jhWlOMnLc0Kk"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# навчання\n",
        "n_epochs = 7\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "   train_loss = train_one_epoch(model_transformer, train_loader, optimizer, criterion, device)\n",
        "   val_loss = evaluate_one_epoch(model_transformer, val_loader, criterion, device)\n",
        "\n",
        "   print(f\"[{epoch+1:02d}] train_loss={train_loss:.4f} | val_loss={val_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVvbS27H0fq8",
        "outputId": "2c4d28a9-fa55-4088-e0fc-59e6cc37fc42"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:515: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)\n",
            "  output = torch._nested_tensor_from_mask(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[01] train_loss=0.5930 | val_loss=0.5310\n",
            "[02] train_loss=0.4855 | val_loss=0.5069\n",
            "[03] train_loss=0.4297 | val_loss=0.4828\n",
            "[04] train_loss=0.3882 | val_loss=0.4776\n",
            "[05] train_loss=0.3574 | val_loss=0.5188\n",
            "[06] train_loss=0.3282 | val_loss=0.4907\n",
            "[07] train_loss=0.3038 | val_loss=0.5085\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Для фінальної перевірки якості\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss, total_acc, n = 0.0, 0, 0\n",
        "    for xb, yb in loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        logits = model(xb)\n",
        "        loss = criterion(logits, yb)\n",
        "        preds = (torch.sigmoid(logits) >= 0.5).long()\n",
        "        total_loss += loss.item() * xb.size(0)\n",
        "        total_acc  += (preds == yb.long()).sum().item()\n",
        "        n += xb.size(0)\n",
        "    return total_loss/n, total_acc/n"
      ],
      "metadata": {
        "id": "dusn5wVifbp3"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = evaluate(model_transformer, test_loader, criterion, device)\n",
        "print(f\"TEST  loss={test_loss:.4f}  acc={test_acc*100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NRb7i14_hvo-",
        "outputId": "5b3e5ef7-df8a-4e60-99b4-4523dd338eaa"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TEST  loss=0.5238  acc=76.77%\n"
          ]
        }
      ]
    }
  ]
}