{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNDxbBxXDGNq+Im1PGsdiiN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/natsakh/IAD/blob/main/Pr_7/7_1_Transformer_Encoder%E2%80%93Decoder_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "dV0TYglwOqLv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Transformer Encoder"
      ],
      "metadata": {
        "id": "aos5kSZGO5PY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(32, 50, 64)\n",
        "# batch=32, seq_len=50, features=d_model=64\n",
        "\n",
        "layer = nn.TransformerEncoderLayer(\n",
        "    d_model=64,\n",
        "    nhead=8,\n",
        "    dim_feedforward=256,\n",
        "    batch_first=True\n",
        ")\n",
        "\n",
        "out = layer(x)   # x shape: [32, 50, 64]\n",
        "\n",
        "print(\"Input:\", x.shape)\n",
        "print(\"Output:\", out.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zm_vXIWHUBxm",
        "outputId": "438d520f-adc5-45a4-9e4f-a7d63e74fedb"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: torch.Size([32, 50, 64])\n",
            "Output: torch.Size([32, 50, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_layer = nn.TransformerEncoderLayer(\n",
        "    d_model=64, nhead=8, dim_feedforward=256, batch_first=True\n",
        ")\n",
        "encoder = nn.TransformerEncoder(encoder_layer, num_layers=4)\n",
        "\n",
        "out = encoder(x)\n",
        "\n",
        "print(\"Input:\", x.shape)\n",
        "print(\"Output:\", out.shape)\n"
      ],
      "metadata": {
        "id": "CSvaN2cfUfLd",
        "outputId": "9373158b-8c9c-4254-db23-e8df5ba6bc9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: torch.Size([32, 50, 64])\n",
            "Output: torch.Size([32, 50, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Decoder only"
      ],
      "metadata": {
        "id": "p_iOBbZ6S9CR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# batch=32, seq_len=20, d_model=64\n",
        "x = torch.rand(32, 20, 64)   # embeddings\n",
        "print(\"Input x shape:\", x.shape)    # [32, 20, 64]\n",
        "\n",
        "# Один шар декодера трансформера\n",
        "decoder_layer = nn.TransformerDecoderLayer(\n",
        "    d_model=64,\n",
        "    nhead=8,\n",
        "    dim_feedforward=256,\n",
        "    batch_first=True,\n",
        ")\n",
        "\n",
        "# Стек із кількох шарів\n",
        "decoder = nn.TransformerDecoder(\n",
        "    decoder_layer,\n",
        "    num_layers=3\n",
        ")\n",
        "\n",
        "# Каузальна маска: кожна позиція \"бачить\" лише попередні\n",
        "B, T, D = x.shape\n",
        "tgt_mask = torch.triu(torch.ones(T, T, dtype=torch.bool), diagonal=1) # True = заборонено\n",
        "#TransformerDecoderLayer приймає дві різні форми масок:\n",
        "#bool mask (True = заборонено)\n",
        "#float mask (-inf = заборонено) mask = torch.triu(torch.ones(T, T) * float('-inf'), diagonal=1)\n",
        "\n",
        "# decoder-only: tgt = emb, memory = emb\n",
        "emb = x\n",
        "\n",
        "out = decoder(\n",
        "    tgt=emb,          # [B, T, D]\n",
        "    memory=emb,       # [B, T, D] – те саме, що і tgt\n",
        "    tgt_mask=tgt_mask\n",
        ")\n",
        "\n",
        "print(\"tgt (emb) shape   :\", emb.shape)\n",
        "print(\"memory shape      :\", emb.shape)\n",
        "print(\"decoder out shape :\", out.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXkfMdrbUu5M",
        "outputId": "4fb2b99c-aa84-4649-fe2e-20ebac330d94"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input x shape: torch.Size([32, 20, 64])\n",
            "tgt (emb) shape   : torch.Size([32, 20, 64])\n",
            "memory shape      : torch.Size([32, 20, 64])\n",
            "decoder out shape : torch.Size([32, 20, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#маска\n",
        "T = 6\n",
        "tgt_mask = torch.triu(torch.ones(T, T, dtype=torch.bool), diagonal=1)\n",
        "print(tgt_mask)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4jpGdMP1WeB7",
        "outputId": "8bae5146-02a1-4214-bfde-591b1a1a85aa"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[False,  True,  True,  True,  True,  True],\n",
            "        [False, False,  True,  True,  True,  True],\n",
            "        [False, False, False,  True,  True,  True],\n",
            "        [False, False, False, False,  True,  True],\n",
            "        [False, False, False, False, False,  True],\n",
            "        [False, False, False, False, False, False]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Encoder Decoder Transformer"
      ],
      "metadata": {
        "id": "Edt5Jc4gYE-t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "B = 32          # batch size\n",
        "S = 10          # довжина вхідної послідовності (source)\n",
        "T = 12          # довжина вихідної послідовності (target)\n",
        "D = 64          # розмір ембедінга (d_model)\n",
        "\n",
        "# ембедінги для src і tgt\n",
        "src = torch.rand(B, S, D)   # [B, S, D]\n",
        "tgt = torch.rand(B, T, D)   # [B, T, D]\n",
        "\n",
        "print(\"src shape:\", src.shape)\n",
        "print(\"tgt shape:\", tgt.shape)\n",
        "\n",
        "# ---- Encoder ----\n",
        "encoder_layer = nn.TransformerEncoderLayer(\n",
        "    d_model=D,\n",
        "    nhead=8,\n",
        "    dim_feedforward=256,\n",
        "    batch_first=True,\n",
        ")\n",
        "encoder = nn.TransformerEncoder(\n",
        "    encoder_layer,\n",
        "    num_layers=2\n",
        ")\n",
        "\n",
        "# ---- Decoder ----\n",
        "decoder_layer = nn.TransformerDecoderLayer(\n",
        "    d_model=D,\n",
        "    nhead=8,\n",
        "    dim_feedforward=256,\n",
        "    batch_first=True,\n",
        ")\n",
        "decoder = nn.TransformerDecoder(\n",
        "    decoder_layer,\n",
        "    num_layers=2\n",
        ")\n",
        "\n",
        "# Каузальна маска для декодера: кожна позиція \"бачить\" лише попередні\n",
        "def generate_subsequent_mask(size, device):\n",
        "    # True = заборонено, False = можна дивитись\n",
        "    mask = torch.triu(\n",
        "        torch.ones(size, size, dtype=torch.bool, device=device),\n",
        "        diagonal=1\n",
        "    )\n",
        "    return mask\n",
        "\n",
        "tgt_mask = generate_subsequent_mask(T, tgt.device)   # [T, T]\n",
        "\n",
        "# ---- Прямий прохід через Encoder + Decoder ----\n",
        "\n",
        "# 1) Пропускаємо src через енкодер → отримуємо memory\n",
        "memory = encoder(src)        # [B, S, D]\n",
        "\n",
        "# 2) Декодер дивиться на tgt (з каузальною маскою) + memory\n",
        "out = decoder(\n",
        "    tgt=tgt,                 # [B, T, D]\n",
        "    memory=memory,           # [B, S, D]\n",
        "    tgt_mask=tgt_mask        # [T, T]\n",
        ")\n",
        "\n",
        "print(\"memory (encoder out) shape:\", memory.shape)\n",
        "print(\"decoder out shape         :\", out.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rMhOE_mYEJ8",
        "outputId": "10be0169-3d28-4428-ae8f-c9a5e52a35ce"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "src shape: torch.Size([32, 10, 64])\n",
            "tgt shape: torch.Size([32, 12, 64])\n",
            "memory (encoder out) shape: torch.Size([32, 10, 64])\n",
            "decoder out shape         : torch.Size([32, 12, 64])\n"
          ]
        }
      ]
    }
  ]
}